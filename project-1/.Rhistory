## set the seed to make the partition reproducible
set.seed(37)
train_ind <- sample(seq_len(nrow(fatmen)), size = smp_size)
men_train = fatmen[train_ind,]
men_test = fatmen[-train_ind,]
# Computing a full model
model = lm(density ~ .-density, data = men_train)
summary(model)
plot(model)
#Finding best reduced models
#ols_step_all_possible(model)
#best_model = ols_step_best_subset(model)
## k-fold cross validation: recall
predict.regsubsets <- function(object, newdata, id,...) {
form <- as.formula(object$call[[2]])
mat <- model.matrix(form, newdata)
coefi <- coef(object, id = id)
xvars <- names(coefi)
mat[,xvars]%*%coefi
}
k = 10
set.seed(37)
folds = sample (1: k , nrow ( men_train ) , replace = TRUE )
cv.errors = matrix(NA, k, 13, dimnames = list(NULL,paste (1:13)))
for(j in 1:k){
best.fit = regsubsets(density~., data = men_train[folds != j,], nvmax = 13)
for(i in 1:13){
pred <- predict(best.fit, men_train[folds == j,] , id = i)
cv.errors[j,i] = mean((men_train$density[folds == j]-pred)^2)
}
}
mean.cv.errors = apply(cv.errors, 2, mean)
par(mfrow = c(1 ,1))
plot(mean.cv.errors, type = 'b')
reg.best = regsubsets(density~., data = men_train, nvmax = 13)
coef(reg.best, 4)
? vif
? Vif
library(car)
? vif
model = lm(density ~ .-density, data = men_train)
summary(model)
plot(model)
? sample
vif(model)
? sample.replace
plot(mean.cv.errors, type = 'b', ylab = 'Mean Cross-Validation Error')
? par
view(mean.cv.errors)
View(mean.cv.errors)
? cbind
? matrix
repeat.cv.errors = matrix(NA, 13, repeats)
repeats = 20
repeat.cv.errors = matrix(NA, 13, repeats)
View(repeat.cv.errors)
repeat.cv.errors[,1] = mean.cv.errors
k = 10 # number of folds
repeats = 20
set.seed(37)
repeat.cv.errors = matrix(NA, 13, repeats)
#TODO: for each iteration r, store mean.cv.error in a matrix
for(r in 1:repeats){
folds = sample(1:k, nrow (men_train), replace = TRUE )
cv.errors = matrix(NA, k, 13, dimnames = list(NULL, paste(1:13)))
for(j in 1:k){
best.fit = regsubsets(density~., data = men_train[folds != j,], nvmax = 13)
for(i in 1:13){
pred <- predict(best.fit, men_train[folds == j,] , id = i)
cv.errors[j,i] = mean((men_train$density[folds == j]-pred)^2)
}
}
mean.cv.errors = apply(cv.errors, 2, mean)
repeat.cv.errors[,r] = mean.cv.errors
}
? apply
mean.repeat.cv.errors = apply(repeat.cv.errors, 1, mean)
View(mean.repeat.cv.errors)
plot(mean.repeat.cv.errors, type = 'b', ylab = 'Mean Cross-Validation Error')
? apply
plot(mean.repeat.cv.errors, type = 'b', ylab = 'Mean Cross-Validation Error')
k = 10 # number of folds
repeats = 20
set.seed(37)
repeat.cv.errors = matrix(NA, 13, repeats)
#TODO: for each iteration r, store mean.cv.error in a matrix
for(r in 1:repeats){
folds = sample(1:k, nrow (men_train), replace = TRUE )
cv.errors = matrix(NA, k, 13, dimnames = list(NULL, paste(1:13)))
for(j in 1:k){
best.fit = regsubsets(density~., data = men_train[folds != j,], nvmax = 13)
for(i in 1:13){
pred <- predict(best.fit, men_train[folds == j,] , id = i)
cv.errors[j,i] = mean((men_train$density[folds == j]-pred)^2)
}
}
mean.cv.errors = apply(cv.errors, 2, mean)
repeat.cv.errors[,r] = mean.cv.errors
}
mean.repeat.cv.errors = apply(repeat.cv.errors, 1, mean)
par(mfrow = c(1 ,1))
plot(mean.repeat.cv.errors, type = 'b', ylab = 'Mean Cross-Validation Error')
rm(list = ls())
SI <- function(men){
# Function to convert the non-SI units to SI
men$weight <- c(1/2.2*men$weight)
men$height <- c(2.54*men$height)
return(men)
}
############ Setting up our data ############
#############################################
fatmen = read.csv("bodyfatmen.csv")
# Converting non-SI units to SI units
fatmen = SI(fatmen)
# Splitting data in one training and one test set
## 80% of the sample size
smp_size <- floor(0.8 * nrow(fatmen))
set.seed(37) # set the seed to make the partition reproducible
train_ind <- sample(seq_len(nrow(fatmen)), size = smp_size)
men_train = fatmen[train_ind,]
men_test = fatmen[-train_ind,]
library(leaps)
library(caret)
library(car)
SI <- function(men){
# Function to convert the non-SI units to SI
men$weight <- c(1/2.2*men$weight)
men$height <- c(2.54*men$height)
return(men)
}
############ Setting up our data ############
#############################################
fatmen = read.csv("bodyfatmen.csv")
# Converting non-SI units to SI units
fatmen = SI(fatmen)
# Splitting data in one training and one test set
## 80% of the sample size
smp_size <- floor(0.8 * nrow(fatmen))
set.seed(37) # set the seed to make the partition reproducible
train_ind <- sample(seq_len(nrow(fatmen)), size = smp_size)
men_train = fatmen[train_ind,]
men_test = fatmen[-train_ind,]
("bodyfatmen.csv")
# Converting non-SI units to SI units
fatmen = SI(fatmen)
SI <- function(men){
# Function to convert the non-SI units to SI
men$weight <- c(1/2.2*men$weight)
men$height <- c(2.54*men$height)
return(men)
}
fatmen = read.csv("bodyfatmen.csv")
setwd("~/GitHub/regression-analysis/project-1")
("bodyfatmen.csv")
# Converting non-SI units to SI units
fatmen = SI(fatmen)
# Splitting data in one training and one test set
## 80% of the sample size
smp_size <- floor(0.8 * nrow(fatmen))
set.seed(37) # set the seed to make the partition reproducible
train_ind <- sample(seq_len(nrow(fatmen)), size = smp_size)
men_train = fatmen[train_ind,]
men_test = fatmen[-train_ind,]
read.csv
read.csv
############ Setting up our data ############
#############################################
fatmen = read.csv("bodyfatmen.csv")
# Converting non-SI units to SI units
fatmen = SI(fatmen)
# Splitting data in one training and one test set
## 80% of the sample size
smp_size <- floor(0.8 * nrow(fatmen))
set.seed(37) # set the seed to make the partition reproducible
train_ind <- sample(seq_len(nrow(fatmen)), size = smp_size)
men_train = fatmen[train_ind,]
men_test = fatmen[-train_ind,]
predict.regsubsets <- function(object, newdata, id,...) {
form <- as.formula(object$call[[2]])
mat <- model.matrix(form, newdata)
coefi <- coef(object, id = id)
xvars <- names(coefi)
mat[,xvars]%*%coefi
}
k = 10 # number of folds
repeats = 20
set.seed(37)
repeat.cv.errors = matrix(NA, 13, repeats)
#TODO: for each iteration r, store mean.cv.error in a matrix
for(r in 1:repeats){
folds = sample(1:k, nrow (men_train), replace = TRUE )
cv.errors = matrix(NA, k, 13, dimnames = list(NULL, paste(1:13)))
for(j in 1:k){
best.fit = regsubsets(density~., data = men_train[folds != j,], nvmax = 13)
for(i in 1:13){
pred <- predict(best.fit, men_train[folds == j,] , id = i)
cv.errors[j,i] = mean((men_train$density[folds == j]-pred)^2)
}
}
mean.cv.errors = apply(cv.errors, 2, mean)
repeat.cv.errors[,r] = mean.cv.errors
}
mean.repeat.cv.errors = apply(repeat.cv.errors, 1, mean)
par(mfrow = c(1 ,1))
plot(mean.repeat.cv.errors, type = 'b', ylab = 'Mean Cross-Validation Error')
reg.best = regsubsets(density~., data = men_train, nvmax = 13)
coef(reg.best, 2)
coef(reg.best, 4)
coef(reg.best, 9)
model.2 = lm(density ~ weight + abdomen, data = men_train)
model.4 = lm(density ~ weight + abdomen + forearm + wrist, data = men_train)
model.9 = lm(density ~ age + weight + neck + abdomen + hip + thigh + ankle + forearm + wrist, data = men_train)
summary(model.2)
summary(model.4)
summary(model.9)
vif(model.2)
vif(model.4)
vif(model.9)
coef(reg.best, 7)
View(fatmen)
summary(model)
############ Computing a full model ############
################################################
model = lm(density ~ .-density, data = men_train)
summary(model)
plot(model)
? influence
## Influential points
summary(influence.measures(model.full))
############ Computing and analyzing full model ############
############################################################
model.full = lm(density ~ .-density, data = men_train)
summary(model.full)
## Influential points
summary(influence.measures(model.full))
## Influential points
influ = influence.measures(model.full)
View(influ)
## Influential points
influ <- influence.measures(model.full)
which(apply(inflm.SR$is.inf, 1, any))
which(apply(influ$is.inf, 1, any))
? influence.measures
? influence
## Influential points
influ <- influence(model.full)
View(influ)
hii = influ$hat
? which
View(model)
p = 13
n = 198
large.hat = which(hii > 2*p/n)
View(large.hat)
View(men_train)
View(men_train)
? cookd
? cooks.distance
cookd(model.full)
library(olsrr)
#Cook's distance
ols_plot_cooksd_chart(model.full)
? ols_plot_cooksd_chart
#DFFITS
ols_plot_dffits(model.full)
? c
#########  k-fold cross validation #########
############################################
predict.regsubsets <- function(object, newdata, id,...) {
form <- as.formula(object$call[[2]])
mat <- model.matrix(form, newdata)
coefi <- coef(object, id = id)
xvars <- names(coefi)
mat[,xvars]%*%coefi
}
# Remove outliers
men_train = men_train[-c(21, 42)]
men_train = fatmen[train_ind,]
men_train = men_train[-c(39, 83)]
# Remove outliers
men_train <- men_train[-c(39, 83)]
# Remove outliers
men_train <- men_train[-c(42, 21)]
# Remove outliers
men_train <- men_train[-c(42, 21),]
men_train = fatmen[train_ind,]
# Remove outliers
men_train <- men_train[-c(42, 21),]
k = 10 # number of folds
repeats = 20
set.seed(37)
repeat.cv.errors = matrix(NA, 13, repeats)
#TODO: for each iteration r, store mean.cv.error in a matrix
for(r in 1:repeats){
folds = sample(1:k, nrow (men_train), replace = TRUE )
cv.errors = matrix(NA, k, 13, dimnames = list(NULL, paste(1:13)))
for(j in 1:k){
best.fit = regsubsets(density~., data = men_train[folds != j,], nvmax = 13)
for(i in 1:13){
pred <- predict(best.fit, men_train[folds == j,] , id = i)
cv.errors[j,i] = mean((men_train$density[folds == j]-pred)^2)
}
}
mean.cv.errors = apply(cv.errors, 2, mean)
repeat.cv.errors[,r] = mean.cv.errors
}
mean.repeat.cv.errors = apply(repeat.cv.errors, 1, mean)
par(mfrow = c(1 ,1))
plot(mean.repeat.cv.errors, type = 'b', ylab = 'Mean Cross-Validation Error')
reg.best = regsubsets(density~., data = men_train, nvmax = 13)
coef(reg.best, 3)
coef(reg.best, 4)
coef(reg.best, 8)
k = 10 # number of folds
repeats = 20
set.seed(77)
repeat.cv.errors = matrix(NA, 13, repeats)
#TODO: for each iteration r, store mean.cv.error in a matrix
for(r in 1:repeats){
folds = sample(1:k, nrow (men_train), replace = TRUE )
cv.errors = matrix(NA, k, 13, dimnames = list(NULL, paste(1:13)))
for(j in 1:k){
best.fit = regsubsets(density~., data = men_train[folds != j,], nvmax = 13)
for(i in 1:13){
pred <- predict(best.fit, men_train[folds == j,] , id = i)
cv.errors[j,i] = mean((men_train$density[folds == j]-pred)^2)
}
}
mean.cv.errors = apply(cv.errors, 2, mean)
repeat.cv.errors[,r] = mean.cv.errors
}
mean.repeat.cv.errors = apply(repeat.cv.errors, 1, mean)
par(mfrow = c(1 ,1))
plot(mean.repeat.cv.errors, type = 'b', ylab = 'Mean Cross-Validation Error')
reg.best = regsubsets(density~., data = men_train, nvmax = 13)
coef(reg.best, 3)
coef(reg.best, 4)
coef(reg.best, 8)
model.3 = lm(density ~ age + abdomen + wrist, data = men_train)
model.4 = lm(density ~ age + height + abdomen + wrist, data = men_train)
model.8 = lm(density ~ age + height + neck + abdomen +
hip + thigh + forearm + wrist, data = men_train)
summary(model.8)
vif(model.3)
vif(model.4)
vif(model.8)
k = 10 # number of folds
repeats = 20
set.seed(37)
repeat.cv.errors = matrix(NA, 13, repeats)
#TODO: for each iteration r, store mean.cv.error in a matrix
for(r in 1:repeats){
folds = sample(1:k, nrow (men_train), replace = TRUE )
cv.errors = matrix(NA, k, 13, dimnames = list(NULL, paste(1:13)))
for(j in 1:k){
best.fit = regsubsets(density~., data = men_train[folds != j,], nvmax = 13)
for(i in 1:13){
pred <- predict(best.fit, men_train[folds == j,] , id = i)
cv.errors[j,i] = mean((men_train$density[folds == j]-pred)^2)
}
}
mean.cv.errors = apply(cv.errors, 2, mean)
repeat.cv.errors[,r] = mean.cv.errors
}
mean.repeat.cv.errors = apply(repeat.cv.errors, 1, mean)
par(mfrow = c(1 ,1))
plot(mean.repeat.cv.errors, type = 'b', ylab = 'Mean Cross-Validation Error')
reg.best = regsubsets(density~., data = men_train, nvmax = 13)
coef(reg.best, 3)
coef(reg.best, 4)
coef(reg.best, 8)
men_train = fatmen[train_ind,]
############ Computing and analyzing full model ############
############################################################
model.full = lm(density ~ .-density, data = men_train)
### Influential points and outliers ###
influ <- influence(model.full)
large.hat = which(hii > 2*p/n) # Shows whichs points are influential
View(large.hat)
#DFFITS
ols_plot_dffits(model.full)
#Cook's distance
ols_plot_cooksd_chart(model.full)
no.outlier.model.full = lm(density ~ .-density, data = men_train)
summary(no.outlier.model.full)
plot(no.outlier.model.full)
no.outlier.model.full = lm(density ~ .-density, data = men_train)
summary(no.outlier.model.full)
# Remove outliers
men_train <- men_train[-c(42, 21),]
no.outlier.model.full = lm(density ~ .-density, data = men_train)
summary(no.outlier.model.full)
plot(no.outlier.model.full)
library(olsrr)
SI <- function(men){
# Function to convert the non-SI units to SI
men$weight <- c(1/2.2*men$weight)
men$height <- c(2.54*men$height)
return(men)
}
############ Setting up our data ############
#############################################
fatmen = read.csv("bodyfatmen.csv")
# Converting non-SI units to SI units
fatmen = SI(fatmen)
# Splitting data in one training and one test set
## 80% of the sample size
smp_size <- floor(0.8 * nrow(fatmen))
set.seed(37) # set the seed to make the partition reproducible
train_ind <- sample(seq_len(nrow(fatmen)), size = smp_size)
men_train_full = fatmen[train_ind,]
men_test_full = fatmen[-train_ind,]
############ Computing and analyzing full model ############
############################################################
model.full = lm(density ~ .-density, data = men_train)
summary(model.full)
plot(model.full)
hii = influ$hat
p = 13
n = 198
large.hat = which(hii > 2*p/n) # Shows whichs points are influential
#Cook's distance
ols_plot_cooksd_chart(model.full)
#DFFITS
ols_plot_dffits(model.full)
# Remove outliers
men_train <- men_train_full[-c(42, 21),]
# Computing full model with outliers removed
no.outlier.model.full = lm(density ~ .-density, data = men_train)
summary(no.outlier.model.full)
plot(no.outlier.model.full)
predict.regsubsets <- function(object, newdata, id,...) {
form <- as.formula(object$call[[2]])
mat <- model.matrix(form, newdata)
coefi <- coef(object, id = id)
xvars <- names(coefi)
mat[,xvars]%*%coefi
}
k = 10 # number of folds
repeats = 20
set.seed(37)
repeat.cv.errors = matrix(NA, 13, repeats)
#TODO: for each iteration r, store mean.cv.error in a matrix
for(r in 1:repeats){
folds = sample(1:k, nrow (men_train), replace = TRUE )
cv.errors = matrix(NA, k, 13, dimnames = list(NULL, paste(1:13)))
for(j in 1:k){
best.fit = regsubsets(density~., data = men_train[folds != j,], nvmax = 13)
for(i in 1:13){
pred <- predict(best.fit, men_train[folds == j,] , id = i)
cv.errors[j,i] = mean((men_train$density[folds == j]-pred)^2)
}
}
mean.cv.errors = apply(cv.errors, 2, mean)
repeat.cv.errors[,r] = mean.cv.errors
}
mean.repeat.cv.errors = apply(repeat.cv.errors, 1, mean)
par(mfrow = c(1 ,1))
plot(mean.repeat.cv.errors, type = 'b', ylab = 'Mean Cross-Validation Error')
### Find that best are 3, 4 and 8 (for our purposes)
reg.best = regsubsets(density~., data = men_train, nvmax = 13)
coef(reg.best, 3)
coef(reg.best, 4)
coef(reg.best, 8)
#########  Computing reduced models #########
#############################################
model.3 = lm(density ~ age + abdomen + wrist, data = men_train)
model.4 = lm(density ~ age + height + abdomen + wrist, data = men_train)
model.8 = lm(density ~ age + height + neck + abdomen +
hip + thigh + forearm + wrist, data = men_train)
summary(model.3)
summary(model.4)
summary(model.8)
vif(model.3)
vif(model.4)
vif(model.8)
############ Computing and analyzing full model ############
############################################################
model.full = lm(density ~ .-density, data = men_train_full)
vif(model.full)
k = 10 # number of folds
repeats = 20
set.seed(37)
repeat.cv.errors = matrix(NA, 13, repeats)
#TODO: for each iteration r, store mean.cv.error in a matrix
for(r in 1:repeats){
folds = sample(1:k, nrow (men_train), replace = TRUE )
cv.errors = matrix(NA, k, 13, dimnames = list(NULL, paste(1:13)))
for(j in 1:k){
best.fit = regsubsets(density~., data = men_train[folds != j,], nvmax = 13)
for(i in 1:13){
pred <- predict(best.fit, men_train[folds == j,] , id = i)
cv.errors[j,i] = mean((men_train$density[folds == j]-pred)^2)
}
}
mean.cv.errors = apply(cv.errors, 2, mean)
repeat.cv.errors[,r] = mean.cv.errors
}
mean.repeat.cv.errors = apply(repeat.cv.errors, 1, mean)
par(mfrow = c(1 ,1))
plot(mean.repeat.cv.errors, type = 'b', ylab = 'Mean Cross-Validation Error')
### Find that best are 3, 4 and 8 (for our purposes)
reg.best = regsubsets(density~., data = men_train, nvmax = 13)
coef(reg.best, 3)
coef(reg.best, 4)
coef(reg.best, 8)
vif(model.3)
vif(model.4)
vif(model.8)
#Cook's distance
ols_plot_cooksd_chart(model.full)
save.image()
